<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>The CUDA programming model - Jiangshan&#039;s Personal Website</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Jiangshan&#039;s Personal Website"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Jiangshan&#039;s Personal Website"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="GPU architecture and CUDA programming model"><meta property="og:type" content="blog"><meta property="og:title" content="The CUDA programming model"><meta property="og:url" content="http://gong208.github.io/2025/04/27/2025-4-27-GPUArchitecture/"><meta property="og:site_name" content="Jiangshan&#039;s Personal Website"><meta property="og:description" content="GPU architecture and CUDA programming model"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://docscontent.nvidia.com/dita/00000186-1a08-d34f-a596-3f291b140000/deeplearning/performance/dl-performance-gpu-background/graphics/simple-gpu-arch.svg"><meta property="article:published_time" content="2025-04-27T23:53:05.000Z"><meta property="article:modified_time" content="2025-05-04T02:00:40.437Z"><meta property="article:author" content="Jiangshan Gong"><meta property="article:tag" content="parallel_programming"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://docscontent.nvidia.com/dita/00000186-1a08-d34f-a596-3f291b140000/deeplearning/performance/dl-performance-gpu-background/graphics/simple-gpu-arch.svg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://gong208.github.io/2025/04/27/2025-4-27-GPUArchitecture/"},"headline":"Jiangshan Gong","image":[],"datePublished":"2025-04-27T23:53:05.000Z","dateModified":"2025-05-04T02:00:40.437Z","author":{"@type":"Person","name":"Jiangshan Gong"},"publisher":{"@type":"Organization","name":"Jiangshan's Personal Website","logo":{"@type":"ImageObject","url":"http://gong208.github.io/img/logo.svg"}},"description":"This is Jiangshan Gong&#39;s personal website"}</script><link rel="canonical" href="http://gong208.github.io/2025/04/27/2025-4-27-GPUArchitecture/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;700&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Jiangshan&#039;s Personal Website" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/Blog">Blog</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/About">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-9-tablet is-9-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-04-27T23:53:05.000Z" title="4/27/2025, 6:53:05 PM">2025-04-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-05-04T02:00:40.437Z" title="5/3/2025, 9:00:40 PM">2025-05-03</time></span><span class="level-item"><a class="link-muted" href="/categories/cs-learning/">cs_learning</a></span><span class="level-item">10 minutes read (About 1512 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">The CUDA programming model</h1><div class="content"><h2 id="1-CPU-vs-GPU"><a href="#1-CPU-vs-GPU" class="headerlink" title="1. CPU vs. GPU"></a>1. CPU vs. GPU</h2><p>Besides a Central Processing Unit (CPU), many computers nowadays also have a Graphic Processing Unit (GPU). Unlike CPU which is designed for sequential programs, the design of GPU is optimized for <strong>highly parallel computations</strong> involved in matrix multiplications during machine learning training process, mining cyptocurrencies, and the graphic performances of video games.</p>
<table>
<thead>
<tr>
<th></th>
<th>CPU: Latency Oriented Design</th>
<th>GPU: Throughput Oriented Design</th>
</tr>
</thead>
<tbody><tr>
<td>Cache</td>
<td>Large cache</td>
<td>Small caches</td>
</tr>
<tr>
<td>Control</td>
<td>Complex flow control</td>
<td>Simple flow control</td>
</tr>
<tr>
<td>ALU</td>
<td>Powerful ALU</td>
<td>Energy efficient ALU</td>
</tr>
</tbody></table>
<p>The CPU aims to reduce the latency of sequential tasks on a single thread. The large cache can reduce the latency of loading the frequently accessed data, the sophisticated flow control and powerful ALU are designed to optimize the latency of the operation with large use of on chip memory and power.<br>In contrast, the GPU aims to devote the chip area and power to the massive floating point calculations. As pointed out by Kirk and Hwu, it takes more effort to reduce latency than to increase throughput based on chip area and power. The design of GPU is to maximize the execution throughput of massive numbers of parallel threads while allowing each thread to higher higher latency. </p>
<p>Since a program always has a mixed of sequential and parallel parts, the winning strategy is to use CPU when the latency is important (for example,<br>when the program is waiting for I&#x2F;O) and use GPU when the throughput is important (for example, when the program is doing matrix multiplications).</p>
<h2 id="2-GPU-architecture"><a href="#2-GPU-architecture" class="headerlink" title="2. GPU architecture"></a>2. GPU architecture</h2><h3 id="Physical-Architecture"><a href="#Physical-Architecture" class="headerlink" title="Physical Architecture"></a>Physical Architecture</h3><!-- image -->
<p><img src="https://docscontent.nvidia.com/dita/00000186-1a08-d34f-a596-3f291b140000/deeplearning/performance/dl-performance-gpu-background/graphics/simple-gpu-arch.svg" alt="GPU Architecture"><br>A GPU mainly consists of multiple Streaming Multiprocessors (SMs), the L2 cache, and the global memory.</p>
<ul>
<li><strong>Streaming Multiprocessor (SM)</strong>: The SM is the basic unit of a GPU. Each SM has warp schedulers, multiple <strong>CUDA cores</strong> (ALUs) each responsible for a thread, an L1 cache, and many other units.</li>
<li><strong>L2 cache</strong>: The L2 cache is a large cache shared by all SMs. It is used to reduce the latency of accessing the global memory.</li>
<li><strong>Global memory</strong>: The global memory is the main memory of the GPU. It is used to store data and instructions for the GPU. The global memory is large but has high latency.</li>
</ul>
<h3 id="Logical-Architecture"><a href="#Logical-Architecture" class="headerlink" title="Logical Architecture"></a>Logical Architecture</h3><h4 id="SPMD-model"><a href="#SPMD-model" class="headerlink" title="SPMD model"></a>SPMD model</h4><p>The GPU launches a <strong>grid</strong> when executing a CUDA kernel. A grid is a 3D array of <strong>blocks</strong> and each block is a 3D array of <strong>threads</strong>. Each thread in a grid has a unique data input executing the same kernel function. This is the <strong>Single Program Multiple Data</strong> (SPMD) model of CUDA.</p>
<h4 id="Identification-of-threads-and-blocks"><a href="#Identification-of-threads-and-blocks" class="headerlink" title="Identification of threads and blocks"></a>Identification of threads and blocks</h4><p>Each thread in a block has a unique thread ID, and each block in a grid has a unique block ID.<br>The thread ID, block ID, and block dimension are used to identify the global location of a thread in a grid. For example, for a kernel with input array of length 2048, when block size is 256, the global location of thread 0 in block 1 is:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> blockSize = <span class="number">256</span>; </span><br><span class="line"><span class="type">int</span> blockIdx = <span class="number">1</span>;</span><br><span class="line"><span class="type">int</span> threadIdx = <span class="number">0</span>;</span><br><span class="line"><span class="type">int</span> globalIdx = blockIdx * blockSize + threadIdx;</span><br></pre></td></tr></table></figure>
<h3 id="Memory-Architecture"><a href="#Memory-Architecture" class="headerlink" title="Memory Architecture"></a>Memory Architecture</h3><table>
<thead>
<tr>
<th>memory hierarchy</th>
<th>permission</th>
<th>latency</th>
<th>scope</th>
<th>lifetime</th>
</tr>
</thead>
<tbody><tr>
<td>Registers</td>
<td>RW</td>
<td>~1 cycle</td>
<td>thread</td>
<td>thread</td>
</tr>
<tr>
<td>shared memory</td>
<td>RW</td>
<td>~5 cycles</td>
<td>block</td>
<td>block</td>
</tr>
<tr>
<td>global memory</td>
<td>RW</td>
<td>~500 cycles</td>
<td>application</td>
<td>application</td>
</tr>
<tr>
<td>constant memory</td>
<td>R</td>
<td>~5 cycles with caching</td>
<td>application</td>
<td>application</td>
</tr>
</tbody></table>
<p>The shared memory resides on the L1 cache of each SM. It is shared among threads in a block.<br>As for the constant memory, it resides on the global memory but is cached in the L1 cache for each thread, achieving much lower latency. </p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>In summary, how does the architecture of GPU work while executing a kernel with multiple threads?</p>
<ol>
<li><h4 id="From-host-to-device"><a href="#From-host-to-device" class="headerlink" title="From host to device"></a>From host to device</h4></li>
</ol>
<p>When a kernel is launched like this from the host:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel&lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt;(args);</span><br></pre></td></tr></table></figure>
<p>The threads are organized according to the grid dimension and block dimension. The CPU sends the kernel launch request to GPU.<br>When GPU receives the request, it loads the instructions for the kernel and starts allocating the necessary memory for the kernel execution and divides the thread blocks across available SMs.</p>
<ol start="2">
<li><h4 id="Inside-SM"><a href="#Inside-SM" class="headerlink" title="Inside SM"></a>Inside SM</h4></li>
</ol>
<p>Each SM has a number of cores (the number depending on specific architectures), and each core is able to process the task of a thread. But before that, the threads are organized into warps. Each warp contains 32 threads. The Warps are scheduled and executed by the warp scheduler.<br>It is important not to confuse the warp with the thread block. A thread block can be larger than a warp, and a warp can contain multiple thread blocks depending on the dimensions. The block dimension is manually controlled by the programmer, while the warp of threads are organized internally.<br>Consecutive threads are typically grouped in one warp. Threads in the same warp are executing the same instruction at the same time. </p>
<blockquote>
<p><strong>branch divergence and predicated execution</strong>:<br>Branch Divergence occurs when threads within the same warp take different execution paths due to control conditions (e.g., if statements). This situation can lead to inefficiencies because the GPU’s SIMD (Single Instruction, Multiple Data) architecture is designed to execute the same instruction across all threads in a warp simultaneously. When threads diverge, the warp must execute each path serially, which can significantly reduce performance.<br>To mitigate the performance impact of branch divergence, GPUs utilize predicated execution. In this model, all threads in a warp evaluate the same instructions regardless of their individual conditions. However, only the threads that meet the condition (i.e., those for which the predicate is true) will write results or perform computations. Threads that do not meet the condition effectively skip the computation without stalling the entire warp</p>
</blockquote>
<p>The block dimension matters for SM occupancy:</p>
<blockquote>
<p>For a SM that can take up to 1536 threads and up to 4 blocks, what block size can achieve full occupancy?<br>For a block size of 256, there will be 1536 &#x2F; 256 &#x3D; 6 blocks, which is more than the maximum of 4 blocks. So the SM will only take 4 blocks and 1024 threads. The occupancy is 1024 &#x2F; 1536 &#x3D; 66.67%.<br>For a block size of 128, there will be 1536 &#x2F; 128 &#x3D; 12 blocks, which is more than the maximum of 4 blocks. So the SM will only take 4 blocks and 512 threads. The occupancy is 512 &#x2F; 1536 &#x3D; 33.33%.<br>For a block size of 384, there will be 1536 &#x2F; 384 &#x3D; 4 blocks, achieving 100% occupancy.</p>
</blockquote>
<p>The warp scheduler inside the SM picks eligible warps. Each clock cycle, the SM issues one instruction per warp scheduler. However, the instruction might not execute right away. The warp might be stalled for multiple reasons like memory access. When a warp is stalled and not making progress, the SM can switch to other ready warps hiding latency.</p>
<h2 id="Related-quizes"><a href="#Related-quizes" class="headerlink" title="Related quizes"></a>Related quizes</h2><h3 id="SP2025-exam1"><a href="#SP2025-exam1" class="headerlink" title="SP2025 exam1"></a><em>SP2025 exam1</em></h3><p>Given a heterogeneous CPU-GPU system, Hrishi and Colin are asked to deploy a benchmark program, which is developed by Vijay, to calibrate the A40 GPU performance.<br>The system has the following characteristics:</p>
<ul>
<li>Contains a single-core CPU and an A40 GPU</li>
<li>The CPU is running under the clock at 3.8GHz, and the GPU is running under the boost mode</li>
<li>Each multiply-add operation takes 3 CPU cycles or 460 GPU cycles per CUDA thread to complete on thi system.</li>
</ul>
<p>The benchmark program has the following characteristics:</p>
<ul>
<li><code>Parallel workload / Sequential workload = 2.00</code></li>
<li>On a CPU only system, both parallel and sequential workloads are done by the CPU, In this CPU-GPU system, the sequential workloads are only done by the CPU, and the parallel workloads are carried out only by the GPU.</li>
<li>In this benchmark, all parallel and sequential workloads are multiply-add operations.</li>
<li>The overall count of multiply-add operations executed on both the CPU-only and CPU-GPU systems is identical.</li>
<li>GPU kernel is launched with <code>blockDim (7, 4, 4)</code> and minimum GridDim for max SM occupancy.</li>
<li>CPU and GPU execution do not overlap, i.e., GPU execution starts only after CPU execution completes and vice versa.</li>
<li>The benchmark program takes 33.12 seconds to finish on the CPU-only system, and you observe an overall speedup of 2.7824 on the CPU-GPU system.</li>
</ul>
<p><em>Question 1</em> With the information above, how many CUDA threads are assigned in each SM?<br>Number of of threads per block: <code>7*4*4 = 112</code>, which needs 4 warps.<br>maximum waprs per SM &#x3D; 48, so maximum blocks per SM limited by number of warps is <code>48 / 4 = 12</code><br>maximum threads per SM &#x3D; 1536, so maximum blocks per SM calculated by number of threads is <code>1536 / 112 = 13.8</code><br>So the number of threads per SM is <code>12 * 112 = 1344</code></p>
<p><em>Question 2</em> Calculate the combined duration of the kernel launch overhead and memory transfer inside the CPU-GPU system.<br>Number of CPU cycles: <code>33.12 * 3.8 = 125.856G</code><br>Number of instructions: <code>125.856 / 3 = 41.952G</code><br>The total number of instructions are the same for CPU-only and CPU-GPU system. According to this and the workload for CPU-GPU system, we can get<br>Number of instruction on GPU: <code>41.95w * 2/3 = 27.968G</code><br>Given that each multiply-add operation takes 460 GPU cycles, we can calculate:<br>Number of GPU cycles: <code>27.968 * 460 = 12865.28G</code><br>Given that under boost mode the GPU runs at 1.74GHz and with total of 84 SMs each SM is running 1344 threads, we can calculate:<br>GPU runtime: <code>12865.28 / (84 * 1344) / 1.74 = 0.06549s</code><br>Total runtime is <code>33.12 / 2.7824 = 11.9034s</code><br>The kernel launch overhead and memory transfer time is the total runtime minus the CPU and GPU runtime, which is <code>11.9034 - 0.06549 - 33.12 / 3 = 11.9034 - 0.06549 - 11.04 = 0.7979s</code></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>The CUDA programming model</p><p><a href="http://gong208.github.io/2025/04/27/2025-4-27-GPUArchitecture/">http://gong208.github.io/2025/04/27/2025-4-27-GPUArchitecture/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Jiangshan Gong</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2025-04-27</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2025-05-03</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/parallel-programming/">parallel_programming</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2025/05/03/2025-05-03-Reduction/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">2025-05-03-Reduction</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2023/11/09/2023-11-9-BinarySearchTree/"><span class="level-item">2023-11-9-BinarySearchTree</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><script src="https://utteranc.es/client.js" repo="gong208/gong208.github.io" issue-term="pathname" label="comment" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="column column-left is-3-tablet is-3-desktop is-3-widescreen  order-1"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-05-04T02:01:33.000Z">2025-05-03</time></p><p class="title"><a href="/2025/05/03/2025-05-03-Reduction/">2025-05-03-Reduction</a></p><p class="categories"><a href="/categories/cs-learning/">cs_learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-04-27T23:53:05.000Z">2025-04-27</time></p><p class="title"><a href="/2025/04/27/2025-4-27-GPUArchitecture/">The CUDA programming model</a></p><p class="categories"><a href="/categories/cs-learning/">cs_learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-11-10T03:05:05.000Z">2023-11-09</time></p><p class="title"><a href="/2023/11/09/2023-11-9-BinarySearchTree/">2023-11-9-BinarySearchTree</a></p><p class="categories"><a href="/categories/cs-learning/">cs_learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-10-17T21:47:58.000Z">2023-10-17</time></p><p class="title"><a href="/2023/10/17/2023-10-17-BinaryTree/">2023-10-17-BinaryTree</a></p><p class="categories"><a href="/categories/cs-learning/">cs_learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-09-26T03:45:36.000Z">2023-09-25</time></p><p class="title"><a href="/2023/09/25/2023-09-25-StackAndQueue/">2023-09-25-StackAndQueue</a></p><p class="categories"><a href="/categories/cs-learning/">cs_learning</a></p></div></article></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/cs-learning/"><span class="level-start"><span class="level-item">cs_learning</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/life/"><span class="level-start"><span class="level-item">life</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/my-dishes/"><span class="level-start"><span class="level-item">my_dishes</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/website-update/"><span class="level-start"><span class="level-item">website_update</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#1-CPU-vs-GPU"><span class="level-left"><span class="level-item">1</span><span class="level-item">1. CPU vs. GPU</span></span></a></li><li><a class="level is-mobile" href="#2-GPU-architecture"><span class="level-left"><span class="level-item">2</span><span class="level-item">2. GPU architecture</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Physical-Architecture"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">Physical Architecture</span></span></a></li><li><a class="level is-mobile" href="#Logical-Architecture"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">Logical Architecture</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#SPMD-model"><span class="level-left"><span class="level-item">2.2.1</span><span class="level-item">SPMD model</span></span></a></li><li><a class="level is-mobile" href="#Identification-of-threads-and-blocks"><span class="level-left"><span class="level-item">2.2.2</span><span class="level-item">Identification of threads and blocks</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Memory-Architecture"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">Memory Architecture</span></span></a></li><li><a class="level is-mobile" href="#Summary"><span class="level-left"><span class="level-item">2.4</span><span class="level-item">Summary</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#From-host-to-device"><span class="level-left"><span class="level-item">2.4.1</span><span class="level-item">From host to device</span></span></a></li><li><a class="level is-mobile" href="#Inside-SM"><span class="level-left"><span class="level-item">2.4.2</span><span class="level-item">Inside SM</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#Related-quizes"><span class="level-left"><span class="level-item">3</span><span class="level-item">Related quizes</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#SP2025-exam1"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">SP2025 exam1</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Jiangshan&#039;s Personal Website" height="28"></a><p class="is-size-7"><span>&copy; 2025 Jiangshan Gong</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>