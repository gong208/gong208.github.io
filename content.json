{"posts":[{"title":"A brand new site looks awkward","text":"Finally I make it, though it embodies a total simplism. Well, it’s fine as long as it is useful… What’s the pupose of this website anyway?","link":"/2022/07/06/2022-07-06-LTNS/"},{"title":"I&#39;m meeting an error this fast?!","text":"I haven’t expected to meet another problem since I struggled to create the website 10min ago.The thing is the description of the posts cannot be displayed, which means my complaints in this post cannot be displayed neither.But I’m still writing to curse this whole pile of not-looking-good codes, which does no good.","link":"/2022/07/06/2022-07-06-damnWebsite/"},{"title":"I&#39;ve passed the Subject 2 exam","text":"Congratulations!! I’m heading to Subject 3 at 60km/h hahaha…","link":"/2022/07/06/2022-07-06-lifeUpdates/"},{"title":"Mastering the website?!","text":"As you can see, I’ve successfully created a ‘category’ page (basically copy-pasting from others’ work though).And here I’m trying to upload the first picture of my website:looks good.","link":"/2022/07/06/2022-07-06-mastering/"},{"title":"image gallery constructed","text":"Agian, I moved a step forward to mastering markdown by using skilfful copy-pasting. Pages log folder log.js log.json log.wxml log.wxss","link":"/2022/07/08/2022-07-08-image_gallery_update/"},{"title":"Testing the images","text":"url: /assets/chisato.jpg url: https://user-images.githubusercontent.com/60023638/177815981-3059b9b2-8228-492e-bc63-2c3d592643e3.jpg I tried to make the picture shown as part of the title of this post, which can then be displayed on homepage fo the website, and failed.another thing worth to mention here is that, the first picture is first uploaded to the assets directory and then refered in this file. In this way, it has to be refered using a relative link. I know the first picture fails to display when you check this file on github, but it works on my website.and the second is directly uploaded to this file, which works well as usual.","link":"/2022/07/07/2022-07-07-addingImage/"},{"title":"Creating a virtual machine (ubuntu) with virtual box","text":"Yesterday, I created a virtual machine for ubuntu system using VirtualBox. Download the iso file of the operating system: Open VirtualBox, then choose the corresponding operating system: Mount the iso file on VirtualBox: Format the disk space. Complete the regular settings. Set network adapters:_ there can be multiple adapters_ For my virtual machine, one adapter is used for NAT, the other one is used to connect to the host through ssh remotely connect to the virtual machine through ssh:On host system, search for folder .ssh . There should be a public key id_rsa.pub .Then open it either using 1cat id_rsa.pub or1vim id_rsa.pub Switch to the virtual machine, again, find .ssh folder and open it with1vim authorized_keys and paste the public key to the file, then save and close the file with1:wq [ENTER] Use1ip a to chech the ip address of the virtual machine, and connect to the virtual machine through ssh remotely.","link":"/2022/07/11/2022-07-11-VirtualMachine/"},{"title":"Inserting multiple rows using executemany command in mysql","text":"While creating a table in a database, a row marked as “AUTO_INCREMENT” is necessary for multiple inserting: 12345678CREATE TABLE university ( id int(8) NOT NULL AUTO_INCREMENT, #without AUTO_INCREMENT, there will be an error while inserting multiple rows name varchar(50) NOT NULL, university varchar(50) NOT NULL, major varchar(50) NOT NULL, PRIMARY KEY (id)) ENGINE=MyISAM AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; Then, create an array of the tuples you want to insert: 123456data = [(&quot;abc&quot;,'四川大学','口腔医学'), (&quot;def&quot;,'北京师范大学','教育学类'), (&quot;ghi&quot;,'北京工业大学','数学类'), (&quot;jkl&quot;,'北京邮电大学','电子信息'), (&quot;mno&quot;,'北京理工大学','未来精工技术'), ] And use executemany command to insert: 123456789try: # 执行sql语句，插入多条数据 cursor.executemany(&quot;insert into university(name, university, major) values (%s, %s, %s)&quot; , data) # 提交数据 db.commit()except: # 发生错误时回滚 db.rollback() print('error') lines about connecting to database and ceating cursor instance is ignored","link":"/2022/07/20/2022-07-21-mysql/"},{"title":"Gong&#39;s Dish -- braising eggplant","text":"My first specialty dishes: Braising eggplant…Even thoug it doesn’t look very tasty, it is really delicious especially when you eat it with rice.Cookbook: Peel the eggplant and “spread” it like this:In this way the dish won’t look that messy. Then steam the eggplant for about 15 min. As for the sauce, there are two versions:_ For the light version, soy sauce 2 spoons oyster sauce 1 spoon rice wine 1 spoon dark soy sauce 1 spoon (or less) honey 1 spoon white sugar, salt depends Starch water 1 bowl_ For the heavy flavor version, soy sauce 2 spoons oyster sauce 1 spoon dark soy sauce 1 spoon (or less) honey 1 spoon white sugar, salt depends Coca Cola (NO PEPSI, thanks!) 1 bowl Fry the eggplants until they turn burnt yellow (I haven’t succeeded frying eggplants into this color, maybe I should add more oil?). Add the sauce into the pan, and stew it for some time over high heat to thicken the sauce. FINISHED さ、やてみろ！","link":"/2022/07/29/2022-07-29-ChineseEggplant/"},{"title":"Long Time No See and some bash codes tips","text":"Hey there, it has been a month since I updated my posts. In the past month I struggled with all those troublesome stuffs emerging at the beginning of the campus life in UIUC.Actually I am still struggling with them nowSo much for the complaints, I’ve enrolled in the cs124 honor course besides the cs124. These two classes are in fact not correlated at all. The cs124 honor is mainly held by a group of senior students.And no credit hour is given for this course. What I can get is, basically, well, honor?All in all, they are training us with bash and git for future projects.I learned bash this week and luckily found that it was just like how the terminal in Linux OS worked.Here are some notes: touch file.ext is used to create a file. The type of the file depends on the extension you give. echo &quot;string&quot; and echo &quot;string&quot; &gt; file.ext The first command prints the content in the string in a new line, and the second one prints the string into the fil. Using the “&gt;” redirect sign, you can write the string directly into the file through terminal, but “&gt;” will delete the original content and “&gt;&gt;” won’t cat file.ext prints the content in the file to the terminalI think these are things to be noticed during my bash learning. So much for that.","link":"/2023/02/01/2022-08-31-Long_Time_No_See/"},{"title":"Winter Holiday Vail, Colorado","text":"Arriving at Vail, Colorado!2022.12.16Heavy snow covered the fields in Denvor!First night at Vail Resort. It was fantastic with all neon lights glowing. Practicing on introductory skiing skills2022.12.17Since I’d skied for once before, I mastered the “snow plough” pretty quickly. snow plough is a elementary skiing skill. It helps to slow down by pushing the back of both skis out, or make a turn by pushing the back of one of the skis. Learning higher level skills and “adventure” in the mountain2022.12.18~2022.12.21To be able to ski on steeper slopes, I had to learn the “parallel turning”, which would slow down and turn more effectively.I practiced on a relatively gentle slope for two days.Here are some tips: Put most of the weight of the body forward (downhill) so that the body won’t fall to the slope; While making a parallel turn, put more weight on the ski downhill, it is then easier to make the ski uphill parallel to the one downhill. Look forward and make sure that you cannot see the tips of your skis, or your weight may be lagging behind, leading to a fall. On 2022.12.21, I rode the chairlift to a top of the mountain and skied all the way down. There were some slip-overs but I did feel a sense of achievement! For more pictures, go to image gallery","link":"/2022/12/22/2022-12-22-holiday/"},{"title":"Wechat Devtool 1","text":"Wechat Miniapplication: Wechat Miniapplication is literally an application based on Wehcat. Different from ordinary applications, it is said that these miniapplications can be used without downloading &amp; installing. In fact, it is because they have a very small size (less than 2M) so that the users won’t be aware of the process of downloading. Wechat developer tool: a specially designed IDE for wechat miniapplications. The regular code structure of a miniapplication: pages It is recommended to create all the codes for each page of the application in this folder. &nbsp log This is an example folder for a log page in the application. In a page folder, there will usually be four files: log.js: .js file is responsible for the logical components of the page. Using javascript, developers can bind events to a button, pass parameters, etc. log.json: I haven't worked much about the .json files so far. As far as I know, it is resonsible for the page's &quot;setup&quot;: page title, text style, background color... log.wxml: .wxml is derived from html. It sues a language syntax similar to html to create elements like `&lt;view&gt;&lt;/view&gt;`, `&lt;button&gt;&lt;/button&gt;` on the page. log.wxss: wxss is derived from css. It works similarly. I use it to arrange the elements shown on the page, like position and flex display. utils util.js app.js app.js file is responsible for the global logic. The APP() in it is the start point of the overall application code. app.json A page can be displayed only after you add the link of the page into this file. app.json is responsible for the global setup. For example, you can set the primary background color or add the tab bars. app.wxss app.wxss works the same as the .wxss files of the pages, but it influences globally. The page wxss arrangement is prior to the global arrangement. project.config.json This file contains the settings of a miniapplication project. sitemap.json","link":"/2023/02/01/2022-09-17-WecahtDevetoolntro/"},{"title":"Implementing multiple nodes pytorch training","text":"Training a model through multiple nodes and mulitple processes 0. IntroductionThis blog passage focuses on implementing distributed data parallelism training among multiple nodes and multiple processes. Three methods, torch.distributed.launch, torchrun, and mpirun are covered. Using WandB to monitor the training process is also included. 1. ResourcesThe model and the dataset are from this blog:https://docs.google.com/document/d/1dSranGJOnn4WotvtogS6MzIdAE9Dda-NIyEFHflSyz8/edit?usp=sharingThe training process refers to this passage:https://lambdalabs.com/blog/multi-node-pytorch-distributed-training-guidehttps://github.com/LambdaLabsML/examples/tree/main/pytorch/distributed/resnet 2. Training environment GPU driver: cuda-keyring_1.0-1Virtual environment: conda 23.5.2Requirements: python==3.10.2wandb==0.15.8tensorboard==2.12.3tensorboard-data-server==0.7.1datasets==1.16.1 3. Preparing the model and datasetWe will deploy the model on two machines as illustrated above, each node is equipped with 8 GPUs. First of all, we have to make sure that the two nodes are connected through ssh. We can set up ip address using the following code on one machine:~$ sudo ip address add 10.10.10.11/24 dev ens11f1and on the other machine:~$ sudo ip address add 10.10.10.11/24 dev ens11f1You can use~$ ip ato check the ip address settings. Then we can use the ssh key to enable ssh connection without a password between the two nodes.Download the Python script from this repo and the dataset as shown in Mao Lei’s article on each machine.To view the information during the training process on WandB, we need to add some pieces of code that upload the parameters we want. First, we have to initiate WandB in the code, but only on the main process on the main node: 12if (LOCAL_RANK == 0 and WORLD_RANK==0): wandb.init(project=project_name, config=argv) Make sure that WandB is initiated only on the main process in the main node and before enumerating the training epochs. LOCAL_RANK is the rank of the local GPUs, from 0 to 7 on each node. You can specify a random one as the main process. WORLD_RANK is the priority of the machines. The main node is ranked 0. In the code, project=project_name sets the project title displayed on WandB. config argument is optional. We can pass some hyperparameters about the model to it. For more information, check out this document about the .init() function. Furthermore, we want to periodically upload parameters like loss rate and accuracy during training, so in the training loop, we can add 12345if LOCAL_RANK == 0: accuracy = evaluate(model=ddp_model, device=device, test_loader=test_loader) torch.save(ddp_model.state_dict(), model_filepath) if WORLD_RANK == 0: wandb.log({'Epoch': epoch,'accuracy': accuracy}) after Line 171 and 12if (LOCAL_RANK == 0 and WORLD_RANK==0): wandb.log({'lr': learning_rate, 'samples': count*batch_size, 'loss/train': loss.item()}) after Line 201.The complete code looks like： main.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240import torchfrom torch.utils.data.distributed import DistributedSamplerfrom torch.utils.data import DataLoaderimport torch.nn as nnimport torch.optim as optimimport torchvisionimport torchvision.transforms as transformsimport wandbimport argparseimport osimport randomimport numpy as npimport timeimport importlibif 'LOCAL_RANK' in os.environ: # Environment variables set by torch.distributed.launch or torchrun LOCAL_RANK = int(os.environ['LOCAL_RANK']) WORLD_SIZE = int(os.environ['WORLD_SIZE']) WORLD_RANK = int(os.environ['RANK'])elif 'OMPI_COMM_WORLD_LOCAL_RANK' in os.environ: # Environment variables set by mpirun LOCAL_RANK = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK']) WORLD_SIZE = int(os.environ['OMPI_COMM_WORLD_SIZE']) WORLD_RANK = int(os.environ['OMPI_COMM_WORLD_RANK'])else: import sys sys.exit(&quot;Can't find the evironment variables for local rank&quot;)def set_random_seeds(random_seed=0): torch.manual_seed(random_seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False np.random.seed(random_seed) random.seed(random_seed)def evaluate(model, device, test_loader): model.eval() correct = 0 total = 0 with torch.no_grad(): for data in test_loader: images, labels = data[0].to(device), data[1].to(device) outputs = model(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() accuracy = correct / total return accuracydef main(): project_name = 'DDP_model' num_epochs_default = 10000 batch_size_default = 256 image_size_default = 224 learning_rate_default = 0.1 random_seed_default = 0 model_dir_default = &quot;saved_models&quot; model_filename_default = &quot;resnet_distributed.pth&quot; steps_syn_default = 20 # Each process runs on 1 GPU device specified by the local_rank argument. parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter) parser.add_argument(&quot;--local-rank&quot;, type=int, help=&quot;Local rank. Necessary for using the torch.distributed.launch utility.&quot;) parser.add_argument(&quot;--num_epochs&quot;, type=int, help=&quot;Number of training epochs.&quot;, default=num_epochs_default) parser.add_argument(&quot;--batch_size&quot;, type=int, help=&quot;Training batch size for one process.&quot;, default=batch_size_default) parser.add_argument(&quot;--image_size&quot;, type=int, help=&quot;Size of input image.&quot;, default=image_size_default) parser.add_argument(&quot;--learning_rate&quot;, type=float, help=&quot;Learning rate.&quot;, default=learning_rate_default) parser.add_argument(&quot;--random_seed&quot;, type=int, help=&quot;Random seed.&quot;, default=random_seed_default) parser.add_argument(&quot;--model_dir&quot;, type=str, help=&quot;Directory for saving models.&quot;, default=model_dir_default) parser.add_argument(&quot;--model_filename&quot;, type=str, help=&quot;Model filename.&quot;, default=model_filename_default) parser.add_argument(&quot;--resume&quot;, action=&quot;store_true&quot;, help=&quot;Resume training from saved checkpoint.&quot;) parser.add_argument(&quot;--backend&quot;, type=str, help=&quot;Backend for distribted training.&quot;, default='nccl', choices=['nccl', 'gloo', 'mpi']) parser.add_argument(&quot;--arch&quot;, type=str, help=&quot;Model architecture.&quot;, default='resnet50', choices=['resnet50', 'resnet18', 'resnet101', 'resnet152']) parser.add_argument(&quot;--use_syn&quot;, action=&quot;store_true&quot;, help=&quot;Use synthetic data&quot;) parser.add_argument(&quot;--steps_syn&quot;, type=int, help=&quot;Step per epoch for training with synthetic data&quot;, default=steps_syn_default) argv = parser.parse_args() local_rank = argv.local_rank num_epochs = argv.num_epochs batch_size = argv.batch_size learning_rate = argv.learning_rate random_seed = argv.random_seed model_dir = argv.model_dir model_filename = argv.model_filename resume = argv.resume backend = argv.backend use_syn = argv.use_syn w = argv.image_size h = argv.image_size c = 3 steps_syn = argv.steps_syn # Create directories outside the PyTorch program # Do not create directory here because it is not multiprocess safe ''' if not os.path.exists(model_dir): os.makedirs(model_dir) ''' if (LOCAL_RANK == 0 and WORLD_RANK==0): wandb.init(project=project_name, config=argv) model_filepath = os.path.join(model_dir, model_filename) # We need to use seeds to make sure that the models initialized in different processes are the same set_random_seeds(random_seed=random_seed) # Initializes the distributed backend which will take care of sychronizing nodes/GPUs torch.distributed.init_process_group(backend=backend, rank=WORLD_RANK, world_size=WORLD_SIZE) # Encapsulate the model on the GPU assigned to the current process model = getattr(torchvision.models, argv.arch)(pretrained=False) device = torch.device(&quot;cuda:{}&quot;.format(LOCAL_RANK)) model = model.to(device) ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK) # We only save the model who uses device &quot;cuda:0&quot; # To resume, the device for the saved model would also be &quot;cuda:0&quot; if resume == True: map_location = {&quot;cuda:0&quot;: &quot;cuda:{}&quot;.format(LOCAL_RANK)} ddp_model.load_state_dict(torch.load(model_filepath, map_location=map_location)) if use_syn: # Synthetic data inputs_syn = torch.rand((batch_size, c, w, h)).to(device) labels_syn = torch.zeros(batch_size, dtype=torch.int64).to(device) else: # Prepare dataset and dataloader transform = transforms.Compose([ transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), ]) # Data should be prefetched # Download should be set to be False, because it is not multiprocess safe train_set = torchvision.datasets.CIFAR10(root=&quot;data&quot;, train=True, download=False, transform=transform) test_set = torchvision.datasets.CIFAR10(root=&quot;data&quot;, train=False, download=False, transform=transform) # Restricts data loading to a subset of the dataset exclusive to the current process train_sampler = DistributedSampler(dataset=train_set) train_loader = DataLoader(dataset=train_set, batch_size=batch_size, sampler=train_sampler, num_workers=8) # Test loader does not have to follow distributed sampling strategy test_loader = DataLoader(dataset=test_set, batch_size=128, shuffle=False, num_workers=8) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(ddp_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5) # Loop over the dataset multiple times times = [] for epoch in range(num_epochs): print(&quot;Local Rank: {}, Epoch: {}, Training ...&quot;.format(LOCAL_RANK, epoch)) # Save and evaluate model routinely if not use_syn: if epoch % 10 == 0: if LOCAL_RANK == 0: accuracy = evaluate(model=ddp_model, device=device, test_loader=test_loader) torch.save(ddp_model.state_dict(), model_filepath) print(&quot;-&quot; * 75) print(&quot;Epoch: {}, Accuracy: {}&quot;.format(epoch, accuracy)) print(&quot;-&quot; * 75) if WORLD_RANK == 0: wandb.log({'Epoch': epoch,'accuracy': accuracy}) ddp_model.train() if use_syn: start_epoch = time.time() for count in range(steps_syn): optimizer.zero_grad() outputs = ddp_model(inputs_syn) loss = criterion(outputs, labels_syn) # print(&quot;***&quot;*10) # print(loss.item()) # print(type(loss.item())) # print(&quot;***&quot;*10) if (LOCAL_RANK == 0 and WORLD_RANK==0): wandb.log({'lr': learning_rate, 'samples': count*batch_size, 'epoch': epoch, 'loss/train': loss.item()}) loss.backward() optimizer.step() torch.cuda.synchronize() end_epoch = time.time() elapsed = end_epoch - start_epoch if epoch &gt; 0: times.append(elapsed) print('num_steps_per_gpu: {}, avg_step_time: {:.4f}'.format(count, elapsed / count)) if (LOCAL_RANK == 0 and WORLD_RANK==0): wandb.log({'epoch': epoch, 'num_steps_per_gpu': count, 'avg_step_time': elapsed/count}) else: train_loader.sampler.set_epoch(epoch) start_epoch = time.time() count = 0 for data in train_loader: inputs, labels = data[0].to(device), data[1].to(device) optimizer.zero_grad() outputs = ddp_model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() count += 1 if (LOCAL_RANK == 0 and WORLD_RANK==0): wandb.log({'lr': learning_rate, 'samples': count*batch_size, 'epoch': epoch, 'loss/train': loss.item()}) torch.cuda.synchronize() end_epoch = time.time() elapsed = end_epoch - start_epoch if epoch &gt; 0: times.append(elapsed) print('num_steps_per_gpu: {}, avg_step_time: {:.4f}'.format(count, elapsed / count)) if (LOCAL_RANK == 0 and WORLD_RANK==0): wandb.log({'epoch': epoch, 'num_steps_per_gpu': count, 'avg_step_time': elapsed / count}) avg_time = sum(times) / (num_epochs - 1) print(&quot;Average epoch time: {}&quot;.format(avg_time))if __name__ == &quot;__main__&quot;: main() 4. Training the model using Distributed Data Parallelism Using torch.distributed.launch In order to use torch.distributed.launch, we need to run the following codes on two machines, respectivelyOn the main node:python3 -m torch.distributed.launch --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=10.10.10.11 --master_port=1234 main.py --backend=nccl --use_syn --batch_size=256 --arch=resnet50On the worker node:python3 -m torch.distributed.launch --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=10.10.10.11 --master_port=1234 main.py --backend=nccl --use_syn --batch_size=256 --arch=resnet50nproc_per_nod defines the number of workers on each node. It should equal to the number of GPUs on each node. nnodes defines the number of nodes.The only difference should be –node_rank. The argument –use_syn means to use synthesized data, we can delete it if we use the dataset. Below are some problems you may encounter: “main.py: error: unrecognized arguments: –local-rank=7”The log reports that the arguments for ranking all the local processes are unrecognized. This may be caused by a mistake in the Python script. We can solve this problem by changing “–local_rank” to “–local-rank” in the code “parser.add_argument(“–local_rank”)”.__ “torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.12GiB (GPU 6; 44.35 GiB total capacity; 7.46 GiB already allocated; 5.80 GiB free; 7.48 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation. See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF” on each GPU.The error reports that the GPUs run out of memory. First, we may want to check if there are other running processes on the GPUs that occupies the memory.~$ nvidia-smiIf there are irrelevant processses running on the GPUs like above, we can kill the processes by~$ sudo killall pythonOr terminate the processes using PID:~$ sudo kill -9 [PID]Sometimes when the previous attempt to train the model didn’t terminate properly, the terminal would report that the port is already in use when we are trying to start another attempt. In this case we can use~$ sudo netstat -lnputto check the process that occupies the port and terminate the process.If the the GPUs still run out of memory even after clearing the processes, we have to consider adjusting the parameters like lowering the batch size and using a smaller model. Adjusting the argument to –batch_size=256, –arch=resnet50 will be valid for the A40 GPUs in this case. Using torchrun Torchrun works similarly to torch.distributed.launch:master:torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=10.10.10.11 --master_port=1234 main.py --backend=nccl --use_syn --batch_size=256 --arch=resnet50worker:torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=10.10.10.11 --master_port=1234 main.py --backend=nccl --use_syn --batch_size=256 --arch=resnet50 Using mpirun Although the above methods work well for DDP on two nodes, they require run command on each node, making it inconvenient when there are more nodes. By contrast, We can launch the DDP training by only typing the command on the master node using mpirun. You can refer to the mpirun section in the passage Multi Node Pytorch Distributed Training Guide for People In a Hurry for installation of OpenMPI and NCCL. Before training, we have to make sure that OpenMPI and NCCL are installed on all of the nodes, and they all use similar virtual environments. To start the DDP training using mpirun, we run the following command on the main node only:mpirun -np 16 -H 10.10.10.11:8,10.10.10.12:8 -x MASTER_ADDR=10.10.10.11 -x MASTER_PORT=1234 -x PATH -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib python3 main.py --backend=nccl --use_syn --batch_size=256 --arch=resnet50The PATH argument ensures that the script runs in the specified environment.","link":"/2023/08/19/2023-08-19-multi-nodes-multi-processes/"},{"title":"Implementing single node, multiple processes LLM training","text":"Training a Python code generator using single node mulitple processes 0. IntroductionThis passage is based on a project form the book Natural Language Processing with Transformers, Chapter 10: Training Transformers from Scratch. The content describes the whole process of training a python code generating model on a remote server and montoring the training parameters on WandB. 1. ResourcesThe dataset and the model are from the book Natural Language Processing with Transformers, Chapter 10: Training Transformers from Scratch.https://huggingface.co/transformersbook/codeparrot-smallInstallation of GPU driver refers to this document:https://docs.google.com/document/d/1BXB90aixWiKry0Qmk-g2PfxdIlJcib_wntOFbqlxnHw/edit 2. The training environment GPU driver: cuda-keyring_1.0-1Virtual environment: conda 23.5.2Requirements: python==3.10.2 wandb==0.15.8 tensorboard==2.12.3 tensorboard-data-server==0.7.1 huggingface-hub==0.16.4 transformers==4.16.2 datasets==1.16.1 accelerate==0.5.1 2. Training process Connect to the remote server.With password:(base) gong208@bogon ~ % ssh clouduser@61.241.103.39 -p 10053Without password:(base) gong208@bogon ~ % cd .ssh(base) gong208@bogon .ssh % ssh-keygen #generates a public key and private key(base) gong208@bogon .ssh % ssh-copy-id -p 10053 clouduer@61.241.103.39 #saves the public key under /.ssh directory of the serverConnect using proxy:(base) gong208@bogon ~ % ssh -R 7890:127.0.0.1:7890 -p 10053 clouduser@61.241.103.39(base) clouduser@ubuntu:~$ export HTTP_PROXY=http://127.0.0.1:7890 HTTPS_PROXY=http://127.0.0.1:7890 Download the model and required dependenciesClone the model codeparrot-small from huggingface using git clone.Create a virtual environment with python3.10.2 using Conda. (env) in the following passage means to run the code in the virtual environment.In the environment, install the dependencies in the requirements.txt file from the model directory:(env) ~/codeparrot-small$ pip install -r requirements.txt problem you may encounter:You may find that the requirements.txt file refers to the newest version of some dependencies, which may cause potential crashes, so you can download using pip and specify the version.You may have to use pip mirror install in China, for example(env) ~/codeparrot-small$ pip install -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com &quot;tensorboard==2.12.3&quot;All the dependencies and their versions are listed below:wandb==0.15.8tensorboard==2.12.3tensorboard-data-server==0.7.1huggingface-hub==0.16.4transformers==4.16.2datasets==1.16.1accelerate==0.5.1 3. Training configurationSince the training script uploads the metrics to WandB, we can log in Wandb before training to receive training information afterward. The training is launched through accelerate. To enable Data Distributed Parallelism, we have to set up some configurations before training:~/codeparrot-small$ accelerate config Compute environment: This machine Which type of machine: multi-GPU How many machines will you use: 1 Use DeepSpeed? NO How many processes in total: 8 Use FP16? yes This configuration is for a single node (one machine) eight processes (8 GPUs) training. With the configuration, we can now try to start the training process. Since it would last for a long time (about 1.5 days for the small model), running the process in tmux is advisable so the training can run in the background.~/codeparrot-small$ tmuxIf you want to detach from the tmux session, press ‘ctrl+b’ then press ‘d’. If you want to end the session, type exit. 4. Launching the training processThen we can launch the training script through accelerate:(env) ~/codeparrot-small$ accelerate launch codeparrot_training.pyTo better record the errors that arise, we can add another piece of code:(env) ~/codeparrot-small$ accelerate launch codeparrot_training.py 2&gt;&amp;1 | tee log.txt, which saves the return information in log.txt.#problems you may encounter:The program may report failing to find the datasets. The most direct solution is to download the dataset from huggingface.~$ git lfs install~$ git clone https://huggingface.co/datasets/transformersbook/codeparrot-train~$ git clone https://huggingface.co/datasets/transformersbook/codeparrot-valid Make sure that the dataset_name is set to ‘../codeparrot’ so that the script can locate the downloaded dataset. There’s an error in the Python script. In line 82, under the valid_data attribute, change the split parameter from ‘train’ to ‘validation’.Delete the error_bad_chunk=False argument in line 78 and line 83, or they may cause a syntax error.","link":"/2023/08/16/2023-08-16-single-node-multi-processes/"},{"title":"2023-09-11-ListADT","text":"Abstract data type(ADT) of LinkedList and ArrayList, code implementations, and comparison. Singly Linked ListBasic data structure:List.h1234567891011#pragma oncetemplate &lt;typename T&gt;class List {private: class ListNode{ T data; *ListNode next; ListNode(T &amp; data): data(data), next(nullptr){}; }; *ListNode head_;} The structure of a ListNode is like: The structure of a LinkedList is like: Basic operations: insert at front; List.hpp123456template &lt;typename T&gt;void List&lt;T&gt;::insertAtFront(const T&amp; data) { ListNode* tmp = new ListNode(data); tmp-&gt;next = head_; head_ = tmp;} Function inserts a new ListNode with the given data at the front of the list. Due to the data structure of a linked list, it is easy to insert at front of a LinkedList. it only takes O(1). We simply need to first create a new ListNode, then make the pointer in the new node point to the current head_. At last, make the head_ pointer point to the new node. _index List.hpp1234567891011template &lt;typename T&gt;List&lt;T&gt;::ListNodeListNode *&amp; List&lt;T&gt;::_index(int index) { return helper(index, head);}template &lt;typename T&gt;List&lt;T&gt;::ListNodeListNode *&amp; List&lt;T&gt;::helper(int index, ListNode *&amp; root) { if (root == nullptr || index == 0) { return root; } return helper(index-1, root-&gt;next);} or List.hpp123456789template &lt;typename T&gt;List&lt;T&gt;::ListNodeListNode *&amp; List&lt;T&gt;::_index(int index) { if (index == 0) {return head_;} ListNode* curr = head_; for (int i = 0; i &lt; index - 1; ++i) { curr = curr-&gt;next; } return curr;} The Function returns the node at the given index by a reference to the pointer. Note that the return type of _index is ListNode *&amp;, a reference to a pointer to a ListNode . In this way, we can change the node the pointer is pointing to in the list in other functions. Both of the recursive or the iterative method need O(n) running time. The iterative method is favorable for lists with bigger size due to the call stack structure of the recursive method. insert List.hpp1234567template &lt;typename T&gt;void List&lt;T&gt;::insert(int index, T data) { ListNode*&amp; curr = _index(index); ListNode* tmp = new ListNode(data); tmp-&gt;next = curr; curr = tmp;} The function inserts a node at given index with given data. First get the reference to the pointer pointing to the node at the given index using _index(index). The rest steps are like insertAtFront(), because it is the reference to a pointer returned from _index(index), changing the reference changes the pointer in the list. random access List.hpp12345template &lt;typename T&gt;T &amp; List&lt;T&gt;::operator[](unsigned index) { ListNode*&amp; curr = _index(index); return curr-&gt;data;} The function returns the data of the node at the given index in the list. Returning the reference to the data considers the needs for a random access. User may hope to change the data at a specific index of a list. find List.hpp123456789template &lt;typename T&gt;ListNode*&amp; List&lt;T&gt;::find(T data) { return helper(head_, data);}template &lt;typename T&gt;ListNode*&amp; List&lt;T&gt;::helper(ListNode *&amp; node, T data) { if (node-&gt;data == data || node == nullptr) {return node;} return helper(node-&gt;next, data);} The function returns the reference to the pointer pointing to the node with given data. The runtime is O(n). remove List.hpp12345678template &lt;typename T&gt;ListNode*&amp; List&lt;T&gt;::remove(ListNode *&amp; node) { Listnode * tmp = node; node = node-&gt;next; T data = temp-&gt;data; delete tmp; return data;} Given the reference to the pointer, removing a ListNode takes O(1) running time. Other removals like remove(T data) or remove(unsigned index) can first use find(data) and _index(index) and then remove the ListNode (the running time increases to O(n)). Pros and Cons of Linked List Pros: it is easy to change the content of the list if we have the address to the nodes. Cons: it is hard to get the address to the nodes, since the linked list is not a contiguous structure, and the only address directly stored is the head_. Array ListBasic data structure:ArrayList.h12345678template &lt;typename T&gt;class ArrayList {public:private: T* data_; T* size; T* capacity;} Implementing the data_, size, and capacity as addresses (pointer) is more direct while inserting/removing items, and it does no harm. Number of items in an array can be calculated by (size - data_)/sizeof(T). Basic operations: random access ArrayList.hpp12345template &lt;typename T&gt;T &amp; ArrayList::operator[](unsigned int index){ T * data = data_ + index*sizeof(T); return *data;} takes O(1) running time. insert at front ArrayList.hpp123456789template &lt;typename T&gt;void ArrayList::insertAtFront(T data){ int back_index = (size - data_) / sizeof(T); for (; back_index &gt; 0; --back_index) { (*this)[back_index] = (*this)[back_index - 1]; } *data_ = data; size++;} ArrayList is not good at inserting at the front. It needs to shift every item backward, which takes O(n) running time. Even thought size is a pointer, size++ works by adding the address with the size of T, automatically pointing to the next available space for type T. insert a given data at a given index ArrayList.hpp123456789template &lt;typename T&gt;void ArrayList::insert(int index, T data){ int back_index = (size - data_) / sizeof(T); for (; back_index &gt; index; --back_index) { (*this)[back_index] = (*this)[back_index - 1]; } data_[index] = data; size++;} Because of the structure of an ArrayList, it is complex to insert at indexes that already have data stored, which takes O(n) running time. push back (insert at frist available space) ArrayList.hpp12345template &lt;typename T&gt;void ArrayList::pushback(T data){ *size = data; size++;} However, if the data is to be stored in an available space in the back,** it takes only O(1) running time. ArrayList is good at inserting at back.** Additionally, popback() also takes only O(1) running time. remove ArrayList.hpp12345678910template &lt;typename T&gt;T ArrayList::remove(int index){ size--; int back_index = (size - data_) / sizeof(T); T data = (*this)[index]; for (; index &lt; back_index; ++index) { (*this)[index] = (*this)[index + 1]; } return data;} Since after removing the item at the index, we need to shift all the following items forward for ArrayList (ArrayList is contiguous), it takes us O(n) to remove. If there are too many items in an ArrayList and we need to remove multiple times, instead of shifting the items and adjusting the size each time, we can first mark all the deleted items as “tombstone” using pointer and complete the shifting and calculating size all by once: number of items = (size - data_)/sizeof(T) - # of tombstones. resizing strategy Since an array is in memory adjacent, which means that it requires continuous memory allocation, once the capacity of an array is allocated, we cannot change it. When an array is growing out of the capacity, we will want to reallocate another array with larger capacity. Here comes the topic of this section: how much larger do we want our new array to be? In the following paragraphs, the +2 resizing strategy (growing the capacity by 2 per reallocation) and the ⨉2 resizing strategy (multiplying the capacity by 2 per reallocation) and the comparison between their runtime are discussed. +2 elements strategy i: reallocation index n: number of copies at a reallocation We can tell that at a reallocation i, the total number of copies we have to make is 2i. In order to encompass n items in an array, starting from an array with capacity 1, we have to increase the capacity by 2 for \\( \\frac{n}{2} \\) times of reallocations. We can then calculate the number of copies we have to make increasing the capacity from 1 to n is $$\\sum_{i=0}^k 2i = k(k+1) = \\frac{n}{2}(\\frac{n}{2}+1)$$ In summary, for the +2 elements resizing strategy, the total number of copies for making n insertins is \\( \\frac{n^2 + 2n}{4} \\), so the expected copies for one insert is \\( \\frac{n+2}{4} \\). ⨉2 elements strategy At a reallocation i, the total number of copies we have to make is \\( 2^i \\). In order to encompass n items, we need to make sure that \\( 2^k &gt; n \\), so the number of reallocation to encompass n items is \\( k=ceil(\\log _{2} n) \\). Total number of copies: $$\\sum_{i=0}^k 2^i = 2^{k+1}-1 = 2^{\\log _{2}{n}+1}-1 = 2n-1$$.For *2 elements strategy the expected cost for one insert is \\( 2-\\frac{1}{n} \\).As shown above, the *2 emelments strategy is more effcient. Array implementationBasic implementations comparison: Sinlgly Linked list Array List Look up arbitrary location (random accessing) O(n) O(1) Inset after given element O(1) Don’t need to find where to insert O(n) Remove a given element O(1) O(n) Insert at arbitrary location O(n) Have to go through the linked list to find where to insert O(n) worst case Remove at arbitrary location O(n) O(n) Search for an input value O(n) O(n) Additional implementations:Can we make our list better at some things? What is the cost? Currently getting the size of a linked list has a Big O of O(n) We can add a private member unsigned size in List class, decreasing the running time of getting the size from O(n) to O(1) The cost is that it increases memory and needs update: memory is increased by 4 bytes, update needs O(1). Currently the Linked List is unsorted Sorting a linked list is applicable when we want a list that returns the smallest item becomes O(1). The cost is that inserting into a sorted list becomes O(n), and some data structures aren’t applicable for sorting. Currently the list is singly linked Making the list doubly linked simplifies removing at back to O(1). Doubly linked list needs an additional pointer at each node, increasing the memory cost.","link":"/2023/09/11/2023-09-11-ListADT/"},{"title":"2023-09-25-StackAndQueue","text":"Abstract data type(ADT) of Stack and Queue. 1. Stack data structureWith the advantage of the first available space at the back, the structure of array is best suited for the implementation of a stack. A stack stores an ordered collection of objects, while the stack data structure only allows inserting or removing an item at the back: push() function puts an item on the top of the stack, and pop() function removes the item at the top of the stack. Stack Abstract Data Structure Order Since the stack only push or pop the item on the top, the item put in later will be pushed out earlier, which means the stack has an order of Last In First Out (LIFO). This order leads to the concept of call stack, which plays an important role in recursion. Implementation C++ has its built-in stack structure, implemented using a vector or a double-ended queue. Runtime Stack.hpp12345template &lt;typename T&gt;void push(T data) { *size = data; size++;} With the pointer size pointing at the first available space at the back, push() takes only O(1) runtime. Stack.hpp1234567template &lt;typename T&gt;T pop() { size- -; T tmp = *size; remove size;return tmp; } To remove the item on top, we must first make the size point to the last item. Then, we copy the data to pop out and remove the item from the list. This function also takes O(1) runtime. 2. Queue Data StructureA queue stores an ordered collection of objects, but we are only allowed to perform two operations: enqueue () puts an item at the back of the queue, and dequeue() removes and returns the front item of the queue. Queue Abstract Data Structure Order Since only inserting at the end and removing at the front are allowed, the item put in the list earlier is also removed earlier, which is a First In First Out (FIFO) order. Implementation Considering that we are going to remove items from the front, a Linked List structure seems to be a good choice for implementation. As for inserting at the front, we can add a tail_ pointer that points to the last item in the list, making enqueue() take O(1) runtime. However, like in the stack structure, the vector or double-ended queue is also used to implement the queue data structure for following reasons: To make an array, we need only one pointer in regardless of the number of items in the list, while we need one pointer for an item to make a linked list. The pointers take more memory and make the operations prone to memory leak. The array uses continuous memory. The whole array can be accessed locally in the RAM (read faster than separated blocked linked list) In order to track the queue behavior within an array, we have to change the size and end from pointer to unsigned int, and add another member: front_. Queue.h12345678910111213#pragma oncetemplate &lt;typename T&gt;class Queue { public: void enqueue(T e); T dequeue(); bool isEmpty(); private: T *data_; unsigned capacity_; unsigned size_; unsigned front_; }; We want to implement a circular queue: Queue.hpp12345678910111213141516171819template &lt;typename T&gt;void Queue::enqueue(T data) { if (size_ &lt; capactiy_) { T * tmp = data_ + sizeof(T)*(size_ + front_)%capacity_; *tmp = data; size_ ++; } }template &lt;typename T&gt;T Queue::dequeue() { if (size==0) return T(); T * tmp = data_ + sizeof(T)*front_; T copy = *tmp; delete tmp; front_ = (front_+1)%capacity; size_- -; return copy;} In order to keep track of the first item in an circular queue, we have to make sure that \\(front_ &lt; 6\\) by making front = (front+1)%6 when increasing front. As for resizing, when we copy the items in the original queue, we have to start from the item at front_ index to item at ((front_+size_)%capacity_ - 1) index to make sure the items stays in the same order.","link":"/2023/09/25/2023-09-25-StackAndQueue/"}],"tags":[{"name":"MACHINELEARNING","slug":"MACHINELEARNING","link":"/tags/MACHINELEARNING/"},{"name":"DATA_STRUCTURE","slug":"DATA-STRUCTURE","link":"/tags/DATA-STRUCTURE/"}],"categories":[{"name":"website_update","slug":"website-update","link":"/categories/website-update/"},{"name":"life","slug":"life","link":"/categories/life/"},{"name":"cs_learning","slug":"cs-learning","link":"/categories/cs-learning/"},{"name":"my_dishes","slug":"my-dishes","link":"/categories/my-dishes/"}],"pages":[{"title":"Welcome to Jiangshan&#39;s Personal Website","text":"WHO AM I","link":"/index.html"},{"title":"About","text":"This page is created according to following links:https://github.com/ppoffice/hexo-theme-icarushttps://www.cnblogs.com/liuxianan/p/build-blog-website-by-hexo-github.html","link":"/About/index.html"},{"title":"Blog","text":"","link":"/Blog/index.html"},{"title":"Home","text":"WHO AM I","link":"/Home/index.html"}]}